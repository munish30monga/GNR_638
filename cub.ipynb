{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **GNR 638:** Machine Learning for Remote Sensing-II\n",
    "### **Mini Project-1:** Fine grained classification on CUB-200-2011 dataset\n",
    "> The task is to train a CNN model with an upper limit of 10M parameters to do fine grained classification on CUB-200-2011 dataset. \n",
    "\n",
    "### Collaborators: \n",
    "[![Munish](https://img.shields.io/badge/22M2153-Munish_Monga-blue)](https://github.com/munish30monga)\n",
    "[![Sachin](https://img.shields.io/badge/22M2162-Sachin_Giroh-darkgreen)](https://github.com/22M2159)\n",
    "\n",
    "### Table of Contents:\n",
    "1. [Introduction](#introduction)\n",
    "2. [Imporing Libraries](#imporing-libraries)\n",
    "3. [Hyperparameters](#hyperparameters)\n",
    "4. [Downloading and Processing CUB Dataset](#downloading-and-processing-cub-dataset)\n",
    "5. [Preparing the Model](#preparing-the-model)\n",
    "6. [Training Loop](#training-loop)\n",
    "7. [Plotting Loss and Accuracy](#plotting-loss-and-accuracy)\n",
    "8.  [References:](#references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imporing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import argparse\n",
    "import yaml\n",
    "import munch\n",
    "import lightning as L\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "import torch\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import lightning as L\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from prettytable import PrettyTable\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from focal_loss.focal_loss import FocalLoss\n",
    "from lightning.pytorch.trainer import Trainer\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, LearningRateMonitor, RichProgressBar\n",
    "from collections import OrderedDict\n",
    "from torchvision.ops import FeaturePyramidNetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"backbone\": 'efficientnet_b0',  # 'efficientnet_b0', 'resnet18', 'dpn48b', 'mobilenetv2_140', 'efficientnet_b2', 'fastvit_s12', 'densenet121', 'mixnet_l'\n",
    "    \"pretrained\": True,\n",
    "    \"unfreeze_last_n\": -1,\n",
    "    \"dataset_dir\": './datasets/cub',\n",
    "    \"batch_size\": 32,\n",
    "    \"num_workers\": 8,\n",
    "    \"img_size\": 512,\n",
    "    \"optimizer\": 'AdamW',  # 'Adam', 'SGD', 'AdamW'\n",
    "    \"scheduler\": 'CosineAnnealing',  # 'CosineAnnealing', 'ReduceLROnPlateau'\n",
    "    \"epochs\": 100,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"temperature\": 0.5,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"patience\": 5,\n",
    "    \"decay_factor\": 0.5,\n",
    "    \"loss_function\": 'CrossEntropy',  # 'CrossEntropy', 'FocalLoss'\n",
    "    \"label_smoothing\": 0.2,\n",
    "    \"gamma\": 1,\n",
    "    \"use_augm\": False,\n",
    "    \"use_fpn\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self, **entries):\n",
    "        self.__dict__.update(entries)\n",
    "        \n",
    "cfg = Config(**hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading and Processing CUB-200-2011 Dataset <a id=\"downloading-and-processing-cub-dataset\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment & run only once for downloading data\n",
    "# !bash down_process_CUB.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentations & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(cfg):\n",
    "    transforms = {\n",
    "        'train': A.Compose([\n",
    "            A.Resize(cfg.img_size, cfg.img_size),\n",
    "            A.CenterCrop(cfg.img_size, cfg.img_size),\n",
    "            A.HorizontalFlip(),\n",
    "            A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
    "            A.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1, p=0.5),\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "            A.Sharpen(alpha=(0.2, 0.5), lightness=(0.5, 1.0), p=0.5),\n",
    "            A.CoarseDropout(max_holes=4, max_height=15, max_width=15, fill_value=0, p=0.3),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2(),\n",
    "        ]),\n",
    "        'val': A.Compose([\n",
    "            A.Resize(cfg.img_size, cfg.img_size),\n",
    "            A.CenterCrop(cfg.img_size, cfg.img_size),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2(),\n",
    "        ]),\n",
    "        'test': A.Compose([\n",
    "            A.Resize(cfg.img_size, cfg.img_size),\n",
    "            A.CenterCrop(cfg.img_size, cfg.img_size),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "    }\n",
    "    return transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUB-200-2011 Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CUB_Dataset(Dataset):\n",
    "    def __init__(self, cfg, split='train', transform=None, split_ratio=0.2):\n",
    "        self.cfg = cfg\n",
    "        self.dataset_dir = Path(self.cfg.dataset_dir)\n",
    "        self.transform = transform\n",
    "        self.split = split\n",
    "        self.split_ratio = split_ratio\n",
    "        self.target2class_dict = {}\n",
    "        self._load_metadata()\n",
    "\n",
    "    def _load_metadata(self):\n",
    "        images = pd.read_csv(self.dataset_dir / 'CUB_200_2011' / 'images.txt', sep=' ', names=['img_id', 'filepath'])\n",
    "        image_class_labels = pd.read_csv(self.dataset_dir / 'CUB_200_2011' / 'image_class_labels.txt', sep=' ', names=['img_id', 'target'])\n",
    "        train_test_split = pd.read_csv(self.dataset_dir / 'CUB_200_2011' / 'train_test_split.txt', sep=' ', names=['img_id', 'is_training_img'])\n",
    "        classes = pd.read_csv(self.dataset_dir / 'CUB_200_2011' / 'classes.txt', sep=' ', names=['class_id', 'class_name'], index_col=False)\n",
    "        self.target2class_dict = pd.Series(classes.class_name.values, index=classes.class_id).to_dict()\n",
    "\n",
    "        data = images.merge(image_class_labels, on='img_id')\n",
    "        data = data.merge(train_test_split, on='img_id')\n",
    "\n",
    "        if self.split == 'train':\n",
    "            self.data = data[data.is_training_img == 1]\n",
    "        else:  # 'test'\n",
    "            self.data = data[data.is_training_img == 0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data.iloc[idx]\n",
    "        path = self.dataset_dir / 'CUB_200_2011' / 'images' / sample.filepath\n",
    "        target = sample.target - 1  # Targets start at 1 by default, so shift to 0\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        img = np.array(img)\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=img)\n",
    "            img = augmented['image']\n",
    "\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUB Dataset Pytorch Lightning Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CUB_DataModule(L.LightningDataModule):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.dataset_dir = Path(self.cfg.dataset_dir)\n",
    "        self.batch_size = self.cfg.batch_size\n",
    "        self.num_workers = self.cfg.num_workers\n",
    "        self.transforms = get_transforms(self.cfg)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if stage in ('fit', None):\n",
    "            self.train_dataset = CUB_Dataset(self.cfg, split='train', transform=self.transforms['train'] if self.cfg.use_augm else self.transforms['val'])\n",
    "        if stage in ('validate', None):\n",
    "            self.val_dataset = CUB_Dataset(self.cfg, split='test', transform=self.transforms['val'])\n",
    "        if stage in ('test', None):\n",
    "            self.test_dataset = CUB_Dataset(self.cfg, split='test', transform=self.transforms['test'])\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_summary(cfg):\n",
    "    print('=> Dataset Summary:')\n",
    "    # Initialize datasets to load their metadata  \n",
    "    train_dataset = CUB_Dataset(cfg, split='train')\n",
    "    test_dataset = CUB_Dataset(cfg, split='test')\n",
    "\n",
    "    # Calculate number of samples for each split\n",
    "    num_samples_train = len(train_dataset)\n",
    "    num_samples_test = len(test_dataset)\n",
    "    total_samples = num_samples_train + num_samples_test\n",
    "    \n",
    "    # Create and fill the table\n",
    "    table = PrettyTable()\n",
    "    table.field_names = [\"Split\", \"Number of Samples\", \"Percentage\"]\n",
    "    \n",
    "    # Calculate and add the percentage for each split\n",
    "    percentage_train = (num_samples_train / total_samples) * 100\n",
    "    percentage_test = (num_samples_test / total_samples) * 100\n",
    "    \n",
    "    table.add_row([\"Train\", num_samples_train, f\"{percentage_train:.2f}%\"])\n",
    "    table.add_row([\"Test\", num_samples_test, f\"{percentage_test:.2f}%\"])\n",
    "    \n",
    "    print(table)\n",
    "    \n",
    "    num_classes = len(set(train_dataset.data['target']))\n",
    "    print(f\"Number of classes: {num_classes}\")\n",
    "    \n",
    "    dataset_summary_dict = {\n",
    "        'train_dataset': train_dataset,\n",
    "        'test_dataset':test_dataset,\n",
    "        'num_classes':num_classes\n",
    "    }\n",
    "    return dataset_summary_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Dataset Summary:\n",
      "+-------+-------------------+------------+\n",
      "| Split | Number of Samples | Percentage |\n",
      "+-------+-------------------+------------+\n",
      "| Train |        5994       |   50.85%   |\n",
      "|  Test |        5794       |   49.15%   |\n",
      "+-------+-------------------+------------+\n",
      "Number of classes: 200\n"
     ]
    }
   ],
   "source": [
    "dataset_summary_dict = dataset_summary(cfg)\n",
    "data_module = CUB_DataModule(cfg)\n",
    "data_module.setup()\n",
    "num_classes = dataset_summary_dict['num_classes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_loss_function(cfg):\n",
    "    if cfg.label_smoothing and cfg.loss_function == 'CrossEntropy':\n",
    "        return nn.CrossEntropyLoss(label_smoothing=cfg.label_smoothing)\n",
    "    \n",
    "    if cfg.loss_function == 'CrossEntropy':\n",
    "        return nn.CrossEntropyLoss()\n",
    "    \n",
    "    if cfg.loss_function == 'FocalLoss':\n",
    "        return FocalLoss(gamma=cfg.gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers & Learning Rate Schedulers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_optimizer_scheduler(cfg, parameters, learning_rate):\n",
    "    optimizer = {\n",
    "        'Adam': torch.optim.Adam(parameters, lr=float(learning_rate)),\n",
    "        'SGD': torch.optim.SGD(parameters, lr=float(learning_rate)),\n",
    "        'AdamW': torch.optim.AdamW(parameters, lr=float(learning_rate), weight_decay=float(cfg.weight_decay)),\n",
    "    }[cfg.optimizer]\n",
    "    print(f\"=> Using '{cfg.optimizer}' optimizer.\")\n",
    "    \n",
    "    scheduler = {\n",
    "        'CosineAnnealing': {\n",
    "            'scheduler': torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=40, eta_min=0),\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1\n",
    "        },\n",
    "        'ReduceLROnPlateau': {\n",
    "            'scheduler': torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=cfg.patience, min_lr=0, factor=cfg.decay_factor),\n",
    "            'monitor': 'val_loss',  \n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1\n",
    "        }\n",
    "    }[cfg.scheduler]\n",
    "    print(f\"=> Using '{cfg.scheduler}' scheduler.\")\n",
    "    return optimizer, scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-grained Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FGCM_Model(L.LightningModule):\n",
    "    def __init__(self, cfg, num_classes):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.learning_rate = cfg.learning_rate\n",
    "        self.save_hyperparameters()  \n",
    "        if cfg.use_fpn:\n",
    "            self.base_model = timm.create_model(self.cfg.backbone, pretrained=cfg.pretrained, features_only=True, num_classes=num_classes)\n",
    "            feature_channels = self.base_model.feature_info.channels()\n",
    "            self.fpn = FeaturePyramidNetwork(\n",
    "                in_channels_list=feature_channels,\n",
    "                out_channels=256,\n",
    "            )\n",
    "            self.projection = nn.Linear(256 * len(feature_channels), num_classes)\n",
    "            # self.projection.apply(self.init_weights)\n",
    "        else:\n",
    "            self.base_model = timm.create_model(self.cfg.backbone, pretrained=cfg.pretrained, num_classes=num_classes)\n",
    "            \n",
    "        self.criterion = choose_loss_function(self.cfg)\n",
    "        \n",
    "        # If unfreeze_last_n is -1, make all layers trainable\n",
    "        if self.cfg.unfreeze_last_n == -1:\n",
    "            print(\"=> All layers are trainable.\")\n",
    "            for param in self.base_model.parameters():\n",
    "                param.requires_grad = True\n",
    "        else:\n",
    "            # Freeze all layers initially\n",
    "            if self.cfg.unfreeze_last_n == 0:\n",
    "                print(\"=> All layers are frozen.\")\n",
    "            else:\n",
    "                print(f\"=> Unfreezing the last {self.cfg.unfreeze_last_n} layers.\")\n",
    "            for param in self.base_model.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "            # Unfreeze the last n layers\n",
    "            num_layers = len(list(self.base_model.children()))\n",
    "            for i, child in enumerate(self.base_model.children()):\n",
    "                if i >= num_layers - self.cfg.unfreeze_last_n:\n",
    "                    for param in child.parameters():\n",
    "                        param.requires_grad = True\n",
    "    \n",
    "    def init_weights(self, layer):\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            torch.nn.init.kaiming_normal_(layer.weight)   \n",
    "                        \n",
    "    def forward(self, x):\n",
    "        if self.cfg.use_fpn:\n",
    "            features = self.base_model(x)\n",
    "            \n",
    "            # Create an OrderedDict for FPN input\n",
    "            fpn_input = OrderedDict([\n",
    "                (f'feat{i}', feature) for i, feature in enumerate(features)\n",
    "            ])\n",
    "            \n",
    "            # FPN Output\n",
    "            fpn_output = self.fpn(fpn_input)\n",
    "            \n",
    "            combined_features = torch.cat([torch.nn.functional.adaptive_avg_pool2d(output, (1, 1)) for output in fpn_output.values()], dim=1)\n",
    "            combined_features = combined_features.view(combined_features.size(0), -1)\n",
    "            \n",
    "            # Final classification\n",
    "            logits = self.projection(combined_features)\n",
    "        else:\n",
    "            x = self.base_model(x)\n",
    "            logits = x / self.cfg.temperature\n",
    "        return logits\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        train_loss = self.criterion(F.softmax(logits, dim=1), y) if self.cfg.loss_function == 'FocalLoss' else self.criterion(logits, y)\n",
    "        self.log('train_loss', train_loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return train_loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(F.softmax(logits, dim=1), y) if self.cfg.loss_function == 'FocalLoss' else self.criterion(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = torch.tensor(torch.sum(preds == y).item() / len(preds), device=self.device)*100\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('val_acc', acc, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return {'val_loss': loss, 'test_acc': acc}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(F.softmax(logits, dim=1), y) if self.cfg.loss_function == 'FocalLoss' else self.criterion(logits, y) \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = torch.tensor(torch.sum(preds == y).item() / len(preds), device=self.device)*100\n",
    "        self.log('test_loss', loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('test_acc', acc, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return {'test_loss': loss, 'test_acc': acc}\n",
    "\n",
    "    def configure_optimizers(self):        \n",
    "        optimizer = {\n",
    "            'Adam': torch.optim.Adam(self.parameters(), lr=float(self.learning_rate)),\n",
    "            'SGD': torch.optim.SGD(self.parameters(), lr=float(self.learning_rate)),\n",
    "            'AdamW': torch.optim.AdamW(self.parameters(), lr=float(self.learning_rate), weight_decay=float(self.cfg.weight_decay)),\n",
    "        }[self.cfg.optimizer]\n",
    "        print(f\"=> Using '{self.cfg.optimizer}' optimizer.\")\n",
    "        \n",
    "        scheduler = {\n",
    "            'CosineAnnealing': {\n",
    "                'scheduler': torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=40, eta_min=0),\n",
    "                'interval': 'epoch',\n",
    "                'frequency': 1\n",
    "            },\n",
    "            'ReduceLROnPlateau': {\n",
    "                'scheduler': torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=self.cfg.patience, min_lr=0, factor=self.cfg.decay_factor),\n",
    "                'monitor': 'val_loss',  \n",
    "                'interval': 'epoch',\n",
    "                'frequency': 1\n",
    "            }\n",
    "        }[self.cfg.scheduler]\n",
    "        print(f\"=> Using '{self.cfg.scheduler}' scheduler.\")\n",
    "        \n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Fine-Grained Classification Model is build using 'efficientnet_b0' as base model.\n",
      "=> All layers are trainable.\n"
     ]
    }
   ],
   "source": [
    "print(f\"=> Fine-Grained Classification Model is build using '{cfg.backbone}' as base model.\")\n",
    "model = FGCM_Model(cfg, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(cfg, model, data_module, logger):  \n",
    "    # Callbacks      \n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath='./checkpoints',\n",
    "        monitor='val_acc',\n",
    "        filename='{cfg.backbone}_{epoch:02d}_{acc:.2f}',\n",
    "        save_top_k=1,\n",
    "        mode='max',\n",
    "        verbose=True,\n",
    "    )\n",
    "    LR_monitor_callback = LearningRateMonitor(\n",
    "        logging_interval='epoch', \n",
    "    )\n",
    "    Rich_pbar_callback = RichProgressBar()\n",
    "     \n",
    "    # Initialize trainer\n",
    "    trainer = Trainer(\n",
    "        max_epochs=cfg.epochs,\n",
    "        log_every_n_steps=1,\n",
    "        callbacks = [\n",
    "            LR_monitor_callback, \n",
    "            checkpoint_callback,\n",
    "            Rich_pbar_callback\n",
    "        ],\n",
    "        logger=logger,\n",
    "        accelerator='gpu',\n",
    "        devices=1,\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.fit(model, datamodule=data_module)\n",
    "    \n",
    "    best_model_path = checkpoint_callback.best_model_path \n",
    "       \n",
    "    return trainer, best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/raid/biplab/munish/miniconda3/envs/GNR_638/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "/raid/biplab/munish/miniconda3/envs/GNR_638/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:652: Checkpoint directory /raid/biplab/munish/GitHub/GNR_638/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Using 'AdamW' optimizer.\n",
      "=> Using 'CosineAnnealing' scheduler.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name       </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ base_model │ EfficientNet     │  4.3 M │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ criterion  │ CrossEntropyLoss │      0 │\n",
       "└───┴────────────┴──────────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName      \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ base_model │ EfficientNet     │  4.3 M │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ criterion  │ CrossEntropyLoss │      0 │\n",
       "└───┴────────────┴──────────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 4.3 M                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 4.3 M                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 17                                                                         \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 4.3 M                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 4.3 M                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 17                                                                         \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9939be51aed4444e8947945a6069d832",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 188: 'val_acc' reached 60.18295 (best 60.18295), saving model to '/raid/biplab/munish/GitHub/GNR_638/checkpoints/cfg.backbone=0_epoch=00_acc=0.00-v7.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 376: 'val_acc' reached 70.52123 (best 70.52123), saving model to '/raid/biplab/munish/GitHub/GNR_638/checkpoints/cfg.backbone=0_epoch=01_acc=0.00-v9.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 564: 'val_acc' reached 75.85433 (best 75.85433), saving model to '/raid/biplab/munish/GitHub/GNR_638/checkpoints/cfg.backbone=0_epoch=02_acc=0.00-v6.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 752: 'val_acc' reached 77.87366 (best 77.87366), saving model to '/raid/biplab/munish/GitHub/GNR_638/checkpoints/cfg.backbone=0_epoch=03_acc=0.00-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 940: 'val_acc' reached 78.96099 (best 78.96099), saving model to '/raid/biplab/munish/GitHub/GNR_638/checkpoints/cfg.backbone=0_epoch=04_acc=0.00-v2.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 1128: 'val_acc' reached 81.20470 (best 81.20470), saving model to '/raid/biplab/munish/GitHub/GNR_638/checkpoints/cfg.backbone=0_epoch=05_acc=0.00-v2.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 1316: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 1504: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 1692: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 1880: 'val_acc' reached 81.86054 (best 81.86054), saving model to '/raid/biplab/munish/GitHub/GNR_638/checkpoints/cfg.backbone=0_epoch=09_acc=0.00.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 2068: 'val_acc' reached 82.13670 (best 82.13670), saving model to '/raid/biplab/munish/GitHub/GNR_638/checkpoints/cfg.backbone=0_epoch=10_acc=0.00-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11, global step 2256: 'val_acc' reached 83.60373 (best 83.60373), saving model to '/raid/biplab/munish/GitHub/GNR_638/checkpoints/cfg.backbone=0_epoch=11_acc=0.00.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12, global step 2444: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13, global step 2632: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14, global step 2820: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15, global step 3008: 'val_acc' reached 83.81084 (best 83.81084), saving model to '/raid/biplab/munish/GitHub/GNR_638/checkpoints/cfg.backbone=0_epoch=15_acc=0.00.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16, global step 3196: 'val_acc' reached 83.91439 (best 83.91439), saving model to '/raid/biplab/munish/GitHub/GNR_638/checkpoints/cfg.backbone=0_epoch=16_acc=0.00.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17, global step 3384: 'val_acc' reached 84.48395 (best 84.48395), saving model to '/raid/biplab/munish/GitHub/GNR_638/checkpoints/cfg.backbone=0_epoch=17_acc=0.00.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18, global step 3572: 'val_acc' reached 84.94995 (best 84.94995), saving model to '/raid/biplab/munish/GitHub/GNR_638/checkpoints/cfg.backbone=0_epoch=18_acc=0.00-v2.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19, global step 3760: 'val_acc' reached 85.51950 (best 85.51950), saving model to '/raid/biplab/munish/GitHub/GNR_638/checkpoints/cfg.backbone=0_epoch=19_acc=0.00-v3.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20, global step 3948: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21, global step 4136: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22, global step 4324: 'val_acc' reached 85.79565 (best 85.79565), saving model to '/raid/biplab/munish/GitHub/GNR_638/checkpoints/cfg.backbone=0_epoch=22_acc=0.00.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23, global step 4512: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24, global step 4700: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25, global step 4888: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26, global step 5076: 'val_acc' reached 85.91647 (best 85.91647), saving model to '/raid/biplab/munish/GitHub/GNR_638/checkpoints/cfg.backbone=0_epoch=26_acc=0.00.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27, global step 5264: 'val_acc' reached 86.00276 (best 86.00276), saving model to '/raid/biplab/munish/GitHub/GNR_638/checkpoints/cfg.backbone=0_epoch=27_acc=0.00-v2.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28, global step 5452: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29, global step 5640: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30, global step 5828: 'val_acc' reached 86.14084 (best 86.14084), saving model to '/raid/biplab/munish/GitHub/GNR_638/checkpoints/cfg.backbone=0_epoch=30_acc=0.00-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31, global step 6016: 'val_acc' reached 86.27891 (best 86.27891), saving model to '/raid/biplab/munish/GitHub/GNR_638/checkpoints/cfg.backbone=0_epoch=31_acc=0.00.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32, global step 6204: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33, global step 6392: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34, global step 6580: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35, global step 6768: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36, global step 6956: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37, global step 7144: 'val_acc' reached 86.39973 (best 86.39973), saving model to '/raid/biplab/munish/GitHub/GNR_638/checkpoints/cfg.backbone=0_epoch=37_acc=0.00-v4.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38, global step 7332: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39, global step 7520: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40, global step 7708: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41, global step 7896: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42, global step 8084: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43, global step 8272: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44, global step 8460: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45, global step 8648: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46, global step 8836: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47, global step 9024: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48, global step 9212: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49, global step 9400: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50, global step 9588: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51, global step 9776: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52, global step 9964: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53, global step 10152: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54, global step 10340: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55, global step 10528: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56, global step 10716: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57, global step 10904: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58, global step 11092: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59, global step 11280: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60, global step 11468: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61, global step 11656: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62, global step 11844: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63, global step 12032: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64, global step 12220: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65, global step 12408: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66, global step 12596: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67, global step 12784: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68, global step 12972: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69, global step 13160: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 70, global step 13348: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71, global step 13536: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72, global step 13724: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 73, global step 13912: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 74, global step 14100: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 75, global step 14288: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 76, global step 14476: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 77, global step 14664: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 78, global step 14852: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 79, global step 15040: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 80, global step 15228: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 81, global step 15416: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 82, global step 15604: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 83, global step 15792: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 84, global step 15980: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 85, global step 16168: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 86, global step 16356: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 87, global step 16544: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 88, global step 16732: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 89, global step 16920: 'val_acc' was not in top 1\n"
     ]
    }
   ],
   "source": [
    "trainer, best_model_path = train_model(cfg, model, data_module, logger=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(best_model_path, data_module, logger):\n",
    "    print(f\"Loading best model from {best_model_path}\")\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = Trainer(\n",
    "        accelerator='gpu',\n",
    "        devices=1,\n",
    "        logger=logger,\n",
    "    )\n",
    "    \n",
    "    # Load the best model\n",
    "    best_model = FGCM_Model.load_from_checkpoint(best_model_path)\n",
    "    \n",
    "    # Run the test using the best model\n",
    "    trainer.test(best_model, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best model from ./checkpoints/cfg.backbone=0_epoch=37_acc=0.00-v4.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/biplab/munish/miniconda3/envs/GNR_638/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> All layers are trainable.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e2dda1305d84d4caef7f564789e40b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     86.39972686767578     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    2.2250165939331055     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    86.39972686767578    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   2.2250165939331055    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_model(best_model_path, data_module, logger=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GNR_638",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
