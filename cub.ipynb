{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **GNR 638:** Machine Learning for Remote Sensing-II\n",
    "### **Mini Project-1:** Fine grained classification on CUB-200-2011 dataset\n",
    "> The task is to train a CNN model with an upper limit of 10M parameters to do fine grained classification on CUB-200-2011 dataset. \n",
    "\n",
    "### Collaborators: \n",
    "[![Munish](https://img.shields.io/badge/22M2153-Munish_Monga-blue)](https://github.com/munish30monga)\n",
    "[![Sachin](https://img.shields.io/badge/22M2162-Sachin_Giroh-darkgreen)](https://github.com/22M2159)\n",
    "\n",
    "### Table of Contents:\n",
    "1. [Introduction](#introduction)\n",
    "2. [Imporing Libraries](#imporing-libraries)\n",
    "3. [Hyperparameters](#hyperparameters)\n",
    "4. [Downloading and Processing CUB Dataset](#downloading-and-processing-cub-dataset)\n",
    "5. [Preparing the Model](#preparing-the-model)\n",
    "6. [Training Loop](#training-loop)\n",
    "7. [Plotting Loss and Accuracy](#plotting-loss-and-accuracy)\n",
    "8.  [References:](#references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imporing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import argparse\n",
    "import yaml\n",
    "import munch\n",
    "import lightning as L\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "import torch\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import lightning as L\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from prettytable import PrettyTable\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from focal_loss.focal_loss import FocalLoss\n",
    "from lightning.pytorch.trainer import Trainer\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, LearningRateMonitor, RichProgressBar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"backbone\": 'efficientnet_b0',  # 'efficientnet_b0', 'resnet18', 'dpn48b', 'mobilenetv2_140', 'efficientnet_b2', 'fastvit_s12', 'densenet121', 'mixnet_l'\n",
    "    \"pretrained\": True,\n",
    "    \"unfreeze_last_n\": -1,\n",
    "    \"dataset_dir\": './datasets/cub',\n",
    "    \"batch_size\": 64,\n",
    "    \"num_workers\": 8,\n",
    "    \"optimizer\": 'Adam',  # 'Adam', 'SGD', 'AdamW'\n",
    "    \"scheduler\": 'CosineAnnealing',  # 'CosineAnnealing', 'ReduceLROnPlateau'\n",
    "    \"epochs\": 20,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"patience\": 5,\n",
    "    \"decay_factor\": 0.5,\n",
    "    \"loss_function\": 'CrossEntropy',  # 'CrossEntropy', 'FocalLoss'\n",
    "    \"label_smoothing\": 0.3,\n",
    "    \"gamma\": 1,\n",
    "    \"use_augm\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self, **entries):\n",
    "        self.__dict__.update(entries)\n",
    "        \n",
    "cfg = Config(**hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading and Processing CUB-200-2011 Dataset <a id=\"downloading-and-processing-cub-dataset\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment & run only once for downloading data\n",
    "# !bash down_process_CUB.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentations & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms():\n",
    "    transforms = {\n",
    "        'train': A.Compose([\n",
    "            A.Resize(224, 224),\n",
    "            A.HorizontalFlip(),\n",
    "            A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
    "            A.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1, p=0.5),\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.5),\n",
    "            A.GaussianBlur(blur_limit=(3, 7), p=0.5),                                        \n",
    "            A.CoarseDropout(max_holes=4, max_height=15, max_width=15, fill_value=0, p=0.3),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2(),\n",
    "        ]),\n",
    "        'val': A.Compose([\n",
    "            A.Resize(224, 224),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2(),\n",
    "        ]),\n",
    "        'test': A.Compose([\n",
    "            A.Resize(224, 224),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "    }\n",
    "    return transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUB-200-2011 Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CUB_Dataset(Dataset):\n",
    "    def __init__(self, dataset_dir, split='train', transform=None, split_ratio=0.2):\n",
    "        self.dataset_dir = Path(dataset_dir)\n",
    "        self.transform = transform\n",
    "        self.split = split\n",
    "        self.split_ratio = split_ratio\n",
    "        self.target2class_dict = {}\n",
    "        self._load_metadata()\n",
    "    \n",
    "    def _load_metadata(self):\n",
    "        images = pd.read_csv(self.dataset_dir / 'CUB_200_2011' / 'images.txt', sep=' ', names=['img_id', 'filepath'])\n",
    "        image_class_labels = pd.read_csv(self.dataset_dir / 'CUB_200_2011' / 'image_class_labels.txt', sep=' ', names=['img_id', 'target'])\n",
    "        train_test_split = pd.read_csv(self.dataset_dir / 'CUB_200_2011' / 'train_test_split.txt', sep=' ', names=['img_id', 'is_training_img'])\n",
    "        classes = pd.read_csv(self.dataset_dir / 'CUB_200_2011' / 'classes.txt', sep=' ', names=['class_id', 'class_name'], index_col=False)\n",
    "        self.target2class_dict = pd.Series(classes.class_name.values, index=classes.class_id).to_dict()\n",
    "\n",
    "        data = images.merge(image_class_labels, on='img_id')\n",
    "        data = data.merge(train_test_split, on='img_id')\n",
    "\n",
    "        if self.split == 'train':\n",
    "            self.data = data[data.is_training_img == 1]\n",
    "        else:  # 'test'\n",
    "            self.data = data[data.is_training_img == 0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data.iloc[idx]\n",
    "        path = self.dataset_dir / 'CUB_200_2011' / 'images' / sample.filepath\n",
    "        target = sample.target - 1  # Targets start at 1 by default, so shift to 0\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        img = np.array(img)\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=img)\n",
    "            img = augmented['image']\n",
    "\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUB Dataset Pytorch Lightning Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CUB_DataModule(L.LightningDataModule):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.dataset_dir = Path(cfg.dataset_dir)\n",
    "        self.batch_size = cfg.batch_size\n",
    "        self.num_workers = cfg.num_workers\n",
    "        self.transforms = get_transforms()\n",
    "        self.cfg = cfg\n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        if stage in ('fit', None):\n",
    "            self.train_dataset = CUB_Dataset(self.dataset_dir, split='train', transform=self.transforms['train'] if self.cfg.use_augm else self.transforms['val'])\n",
    "        if stage in ('validate', None):\n",
    "            self.val_dataset = CUB_Dataset(self.dataset_dir, split='test', transform=self.transforms['val'])\n",
    "        if stage in ('test', None):\n",
    "            self.test_dataset = CUB_Dataset(self.dataset_dir, split='test', transform=self.transforms['test'])\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_summary(dataset_dir):\n",
    "    print('=> Dataset Summary:')\n",
    "    # Initialize datasets to load their metadata  \n",
    "    train_dataset = CUB_Dataset(dataset_dir, split='train')\n",
    "    test_dataset = CUB_Dataset(dataset_dir, split='test')\n",
    "\n",
    "    # Calculate number of samples for each split\n",
    "    num_samples_train = len(train_dataset)\n",
    "    num_samples_test = len(test_dataset)\n",
    "    total_samples = num_samples_train + num_samples_test\n",
    "    \n",
    "    # Create and fill the table\n",
    "    table = PrettyTable()\n",
    "    table.field_names = [\"Split\", \"Number of Samples\", \"Percentage\"]\n",
    "    \n",
    "    # Calculate and add the percentage for each split\n",
    "    percentage_train = (num_samples_train / total_samples) * 100\n",
    "    percentage_test = (num_samples_test / total_samples) * 100\n",
    "    \n",
    "    table.add_row([\"Train\", num_samples_train, f\"{percentage_train:.2f}%\"])\n",
    "    table.add_row([\"Test\", num_samples_test, f\"{percentage_test:.2f}%\"])\n",
    "    \n",
    "    print(table)\n",
    "    \n",
    "    num_classes = len(set(train_dataset.data['target']))\n",
    "    print(f\"Number of classes: {num_classes}\")\n",
    "    \n",
    "    dataset_summary_dict = {\n",
    "        'train_dataset': train_dataset,\n",
    "        'test_dataset':test_dataset,\n",
    "        'num_classes':num_classes\n",
    "    }\n",
    "    return dataset_summary_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Dataset Summary:\n",
      "+-------+-------------------+------------+\n",
      "| Split | Number of Samples | Percentage |\n",
      "+-------+-------------------+------------+\n",
      "| Train |        5994       |   50.85%   |\n",
      "|  Test |        5794       |   49.15%   |\n",
      "+-------+-------------------+------------+\n",
      "Number of classes: 200\n"
     ]
    }
   ],
   "source": [
    "dataset_summary_dict = dataset_summary(cfg.dataset_dir)\n",
    "data_module = CUB_DataModule(cfg)\n",
    "data_module.setup()\n",
    "num_classes = dataset_summary_dict['num_classes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLossWithSmoothing(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_classes: int,\n",
    "            gamma: int = 1,\n",
    "            lb_smooth: float = 0.1,\n",
    "            size_average: bool = True,\n",
    "            ignore_index: int = None,\n",
    "            alpha: float = None):\n",
    "  \n",
    "        super(FocalLossWithSmoothing, self).__init__()\n",
    "        self._num_classes = num_classes\n",
    "        self._gamma = gamma\n",
    "        self._lb_smooth = lb_smooth\n",
    "        self._size_average = size_average\n",
    "        self._ignore_index = ignore_index\n",
    "        self._log_softmax = nn.LogSoftmax(dim=1)\n",
    "        self._alpha = alpha\n",
    "\n",
    "        if self._num_classes <= 1:\n",
    "            raise ValueError('The number of classes must be 2 or higher')\n",
    "        if self._gamma < 0:\n",
    "            raise ValueError('Gamma must be 0 or higher')\n",
    "        if self._alpha is not None:\n",
    "            if self._alpha <= 0 or self._alpha >= 1:\n",
    "                raise ValueError('Alpha must be 0 <= alpha <= 1')\n",
    "\n",
    "    def forward(self, logits, label):\n",
    "        \"\"\"\n",
    "        :param logits: (batch_size, class, height, width)\n",
    "        :param label:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        logits = logits.float()\n",
    "        difficulty_level = self._estimate_difficulty_level(logits, label)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            label = label.clone().detach()\n",
    "            if self._ignore_index is not None:\n",
    "                ignore = label.eq(self._ignore_index)\n",
    "                label[ignore] = 0\n",
    "            lb_pos, lb_neg = 1. - self._lb_smooth, self._lb_smooth / (self._num_classes - 1)\n",
    "            lb_one_hot = torch.empty_like(logits).fill_(\n",
    "                lb_neg).scatter_(1, label.unsqueeze(1), lb_pos).detach()\n",
    "        logs = self._log_softmax(logits)\n",
    "        loss = -torch.sum(difficulty_level * logs * lb_one_hot, dim=1)\n",
    "        if self._ignore_index is not None:\n",
    "            loss[ignore] = 0\n",
    "        return loss.mean()\n",
    "\n",
    "    def _estimate_difficulty_level(self, logits, label):\n",
    "        \"\"\"\n",
    "        :param logits:\n",
    "        :param label:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        one_hot_key = torch.nn.functional.one_hot(label, num_classes=self._num_classes)\n",
    "        if len(one_hot_key.shape) == 4:\n",
    "            one_hot_key = one_hot_key.permute(0, 3, 1, 2)\n",
    "        if one_hot_key.device != logits.device:\n",
    "            one_hot_key = one_hot_key.to(logits.device)\n",
    "        pt = one_hot_key * F.softmax(logits, dim=1)\n",
    "        difficulty_level = torch.pow(1 - pt, self._gamma)\n",
    "        return difficulty_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_loss_function(cfg, num_classes):\n",
    "    if cfg.label_smoothing and cfg.loss_function == 'CrossEntropy':\n",
    "        return nn.CrossEntropyLoss(label_smoothing=cfg.label_smoothing)\n",
    "    \n",
    "    if cfg.loss_function == 'CrossEntropy':\n",
    "        return nn.CrossEntropyLoss()\n",
    "    \n",
    "    if cfg.loss_function == 'FocalLoss' and cfg.label_smoothing:\n",
    "        return FocalLossWithSmoothing(num_classes=num_classes, gamma=cfg.gamma, lb_smooth=cfg.label_smoothing)\n",
    "    \n",
    "    if cfg.loss_function == 'FocalLoss':\n",
    "        return FocalLoss(gamma=cfg.gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers & Learning Rate Schedulers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_optimizer_scheduler(cfg, parameters, learning_rate):\n",
    "    optimizer = {\n",
    "        'Adam': torch.optim.Adam(parameters, lr=float(learning_rate)),\n",
    "        'SGD': torch.optim.SGD(parameters, lr=float(learning_rate)),\n",
    "        'AdamW': torch.optim.AdamW(parameters, lr=float(learning_rate), weight_decay=float(cfg.weight_decay)),\n",
    "    }[cfg.optimizer]\n",
    "    print(f\"=> Using '{cfg.optimizer}' optimizer.\")\n",
    "    \n",
    "    scheduler = {\n",
    "        'CosineAnnealing': {\n",
    "            'scheduler': torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=40, eta_min=0),\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1\n",
    "        },\n",
    "        'ReduceLROnPlateau': {\n",
    "            'scheduler': torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=cfg.patience, min_lr=0, factor=cfg.decay_factor),\n",
    "            'monitor': 'val_loss',  \n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1\n",
    "        }\n",
    "    }[cfg.scheduler]\n",
    "    print(f\"=> Using '{cfg.scheduler}' scheduler.\")\n",
    "    return optimizer, scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-grained Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FGCM_Model(L.LightningModule):\n",
    "    def __init__(self, cfg, num_classes):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.learning_rate = cfg.learning_rate\n",
    "        self.save_hyperparameters()  \n",
    "        self.base_model = timm.create_model(self.cfg.backbone, pretrained=cfg.pretrained, num_classes=num_classes)\n",
    "        self.criterion = choose_loss_function(self.cfg, num_classes)\n",
    "             \n",
    "        # If unfreeze_last_n is -1, make all layers trainable\n",
    "        if self.cfg.unfreeze_last_n == -1:\n",
    "            for param in self.base_model.parameters():\n",
    "                param.requires_grad = True\n",
    "        else:\n",
    "            for param in self.base_model.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "            # Unfreeze the last n layers\n",
    "            num_layers = len(list(self.base_model.children()))\n",
    "            for i, child in enumerate(self.base_model.children()):\n",
    "                if i >= num_layers - self.cfg.unfreeze_last_n:\n",
    "                    for param in child.parameters():\n",
    "                        param.requires_grad = True\n",
    "    \n",
    "    def init_weights(self, layer):\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            torch.nn.init.kaiming_normal_(layer.weight)   \n",
    "                        \n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)\n",
    "        return x\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        train_loss = self.criterion(F.softmax(logits, dim=1), y) if self.cfg.loss_function == 'FocalLoss' else self.criterion(logits, y)\n",
    "        self.log('train_loss', train_loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return train_loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(F.softmax(logits, dim=1), y) if self.cfg.loss_function == 'FocalLoss' else self.criterion(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = torch.tensor(torch.sum(preds == y).item() / len(preds), device=self.device)*100\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('val_acc', acc, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return {'val_loss': loss, 'test_acc': acc}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(F.softmax(logits, dim=1), y) if self.cfg.loss_function == 'FocalLoss' else self.criterion(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = torch.tensor(torch.sum(preds == y).item() / len(preds), device=self.device)*100\n",
    "        self.log('test_loss', loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('test_acc', acc, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return {'test_loss': loss, 'test_acc': acc}\n",
    "\n",
    "    def configure_optimizers(self):        \n",
    "        optimizer = {\n",
    "            'Adam': torch.optim.Adam(self.parameters(), lr=float(self.learning_rate)),\n",
    "            'SGD': torch.optim.SGD(self.parameters(), lr=float(self.learning_rate)),\n",
    "            'AdamW': torch.optim.AdamW(self.parameters(), lr=float(self.learning_rate), weight_decay=float(self.cfg.weight_decay)),\n",
    "        }[self.cfg.optimizer]\n",
    "        print(f\"=> Using '{self.cfg.optimizer}' optimizer.\")\n",
    "        \n",
    "        scheduler = {\n",
    "            'CosineAnnealing': {\n",
    "                'scheduler': torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=40, eta_min=0),\n",
    "                'interval': 'epoch',\n",
    "                'frequency': 1\n",
    "            },\n",
    "            'ReduceLROnPlateau': {\n",
    "                'scheduler': torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=self.cfg.patience, min_lr=0, factor=self.cfg.decay_factor),\n",
    "                'monitor': 'val_loss',  \n",
    "                'interval': 'epoch',\n",
    "                'frequency': 1\n",
    "            }\n",
    "        }[self.cfg.scheduler]\n",
    "        print(f\"=> Using '{self.cfg.scheduler}' scheduler.\")\n",
    "        \n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Fine-Grained Classification Model is build using 'efficientnet_b0' as base model.\n"
     ]
    }
   ],
   "source": [
    "print(f\"=> Fine-Grained Classification Model is build using '{cfg.backbone}' as base model.\")\n",
    "model = FGCM_Model(cfg, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(cfg, model, data_module, logger):  \n",
    "    # Callbacks      \n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath='./checkpoints',\n",
    "        monitor='val_acc',\n",
    "        filename='{cfg.backbone}_{epoch:02d}_{acc:.2f}',\n",
    "        save_top_k=1,\n",
    "        mode='max',\n",
    "        verbose=True,\n",
    "    )\n",
    "    LR_monitor_callback = LearningRateMonitor(\n",
    "        logging_interval='epoch', \n",
    "    )\n",
    "    Rich_pbar_callback = RichProgressBar()\n",
    "     \n",
    "    # Initialize trainer\n",
    "    trainer = Trainer(\n",
    "        max_epochs=cfg.epochs,\n",
    "        log_every_n_steps=1,\n",
    "        callbacks = [\n",
    "            LR_monitor_callback, \n",
    "            checkpoint_callback,\n",
    "            # Rich_pbar_callback\n",
    "        ],\n",
    "        logger=logger,\n",
    "        accelerator='gpu',\n",
    "        devices=1,\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.fit(model, datamodule=data_module)\n",
    "    \n",
    "  \n",
    "    best_model_path = checkpoint_callback.best_model_path\n",
    "    \n",
    "    return trainer, best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/raid/biplab/munish/miniconda3/envs/GNR_638/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:652: Checkpoint directory /raid/biplab/munish/GitHub/GNR_638/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "  | Name       | Type             | Params\n",
      "------------------------------------------------\n",
      "0 | base_model | EfficientNet     | 4.3 M \n",
      "1 | criterion  | CrossEntropyLoss | 0     \n",
      "------------------------------------------------\n",
      "4.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.3 M     Total params\n",
      "17.055    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Using 'Adam' optimizer.\n",
      "=> Using 'CosineAnnealing' scheduler.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ee961bffcb04627b433a2778af94498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d4b6a483c1d4fec8fa976ffd8ffad83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62bbc10e72b345d7882befe126ec40f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 94: 'val_acc' reached 54.24577 (best 54.24577), saving model to '/raid/biplab/munish/GitHub/GNR_638/checkpoints/cfg.backbone=0_epoch=00_acc=0.00-v3.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7690bee203fd4460b3de41da868d566d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 188: 'val_acc' reached 63.94546 (best 63.94546), saving model to '/raid/biplab/munish/GitHub/GNR_638/checkpoints/cfg.backbone=0_epoch=01_acc=0.00-v2.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6688f6ecaf91463c98d73b36d9451b59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 282: 'val_acc' reached 67.46635 (best 67.46635), saving model to '/raid/biplab/munish/GitHub/GNR_638/checkpoints/cfg.backbone=0_epoch=02_acc=0.00-v3.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d848c06fc824c39a786ec65ef3c52de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 376: 'val_acc' reached 70.00345 (best 70.00345), saving model to '/raid/biplab/munish/GitHub/GNR_638/checkpoints/cfg.backbone=0_epoch=03_acc=0.00-v1.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad2755c918b24016b94576706d15eb3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 470: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e9895620ebf45888d17ce0b5e03df66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 564: 'val_acc' reached 71.88470 (best 71.88470), saving model to '/raid/biplab/munish/GitHub/GNR_638/checkpoints/cfg.backbone=0_epoch=05_acc=0.00-v2.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57823bd59fd546a18d52ea338260f342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 658: 'val_acc' reached 73.02382 (best 73.02382), saving model to '/raid/biplab/munish/GitHub/GNR_638/checkpoints/cfg.backbone=0_epoch=06_acc=0.00.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8f1c42dd25d4b839524ec1ea28ee4c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 752: 'val_acc' reached 73.07559 (best 73.07559), saving model to '/raid/biplab/munish/GitHub/GNR_638/checkpoints/cfg.backbone=0_epoch=07_acc=0.00-v2.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2079583f93a744dbbe476426c664c99c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 846: 'val_acc' reached 73.50708 (best 73.50708), saving model to '/raid/biplab/munish/GitHub/GNR_638/checkpoints/cfg.backbone=0_epoch=08_acc=0.00-v1.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7b17c1489414aa4866d51c28f0cb7ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 940: 'val_acc' reached 73.92130 (best 73.92130), saving model to '/raid/biplab/munish/GitHub/GNR_638/checkpoints/cfg.backbone=0_epoch=09_acc=0.00.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9625854be01947259bf6b95e50429d49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 1034: 'val_acc' reached 75.25026 (best 75.25026), saving model to '/raid/biplab/munish/GitHub/GNR_638/checkpoints/cfg.backbone=0_epoch=10_acc=0.00-v1.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3c29a5116b04c638448c55c75568e76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11, global step 1128: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3bd60d7ab9240f988178699ed91d1a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12, global step 1222: 'val_acc' reached 75.78529 (best 75.78529), saving model to '/raid/biplab/munish/GitHub/GNR_638/checkpoints/cfg.backbone=0_epoch=12_acc=0.00-v1.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d69131624c35495f9b107079dfa3f5d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13, global step 1316: 'val_acc' reached 76.42389 (best 76.42389), saving model to '/raid/biplab/munish/GitHub/GNR_638/checkpoints/cfg.backbone=0_epoch=13_acc=0.00-v1.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98f2538902df46d3b8c2918dc9e22d62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14, global step 1410: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1badfa0d1d124a01a1fd32de268fce40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15, global step 1504: 'val_acc' reached 76.88988 (best 76.88988), saving model to '/raid/biplab/munish/GitHub/GNR_638/checkpoints/cfg.backbone=0_epoch=15_acc=0.00.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8028c989c24b4749863660c64debb608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16, global step 1598: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cf1043291a746038a1b73c955293541",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17, global step 1692: 'val_acc' reached 77.01070 (best 77.01070), saving model to '/raid/biplab/munish/GitHub/GNR_638/checkpoints/cfg.backbone=0_epoch=17_acc=0.00.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d865921d9d9466ab59ec110dc55d59c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18, global step 1786: 'val_acc' reached 77.40766 (best 77.40766), saving model to '/raid/biplab/munish/GitHub/GNR_638/checkpoints/cfg.backbone=0_epoch=18_acc=0.00-v1.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "310d64b8afd745179f45a5e5e99418e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19, global step 1880: 'val_acc' was not in top 1\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer, best_model_path = train_model(cfg, model, data_module, logger=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(best_model_path, data_module):\n",
    "    print(f\"Loading best model.\")\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = Trainer(\n",
    "        accelerator='gpu',\n",
    "        devices=1,\n",
    "    )\n",
    "    \n",
    "    # Load the best model\n",
    "    best_model = FGCM_Model.load_from_checkpoint(best_model_path)\n",
    "    \n",
    "    # Run the test using the best model\n",
    "    trainer.test(best_model, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9531cc08d7d44468af3f9d2a7404cdb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     77.40766143798828     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     2.984273910522461     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    77.40766143798828    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    2.984273910522461    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_model(best_model_path, data_module)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GNR_638",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
